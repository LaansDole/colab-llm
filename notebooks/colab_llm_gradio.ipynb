{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Gradio ChatUI for Google Colab\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/LaansDole/colab-llm/blob/main/colab_ui_enhanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This notebook provides an enhanced chat interface for interacting with your local LLM in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Setup Requirements\n",
    "\n",
    "First, we'll install all necessary packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c92ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
    "\n",
    "#@markdown Press play on the music player that will appear below:\n",
    "%%html\n",
    "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection - Change this to your preferred model\n",
    "MODEL_NAME = \"maryasov/qwen2.5-coder-cline:7b-instruct-q8_0\"\n",
    "\n",
    "# Set environment variables\n",
    "%env OLLAMA_CONTEXT_LENGTH=16384\n",
    "%env OLLAMA_HOST=0.0.0.0\n",
    "%env OLLAMA_KEEP_ALIVE=-1\n",
    "\n",
    "# Install required system packages\n",
    "!apt-get install -y lshw pciutils\n",
    "\n",
    "# Check CUDA and GPU\n",
    "!nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available RAM\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print(f\"\\nüß† Available RAM: {ram_gb:.1f} GB\")\n",
    "print(\"‚úÖ High-RAM runtime!\" if ram_gb >= 20 else \"‚ùå Not a high-RAM runtime.\")\n",
    "\n",
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Install Gradio and other dependencies\n",
    "!pip install -q gradio==4.14.0 markdown rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 2: Start Ollama and Set Up Cloudflare Tunnel\n",
    "\n",
    "This cell starts the Ollama server and creates a Cloudflare tunnel to make it accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "import re\n",
    "\n",
    "# Function to start ollama in a background thread\n",
    "def start_ollama():\n",
    "    subprocess.call(['ollama', 'serve'])\n",
    "\n",
    "# Start Ollama server\n",
    "print(\"üì° Starting Ollama server...\")\n",
    "ollama_thread = threading.Thread(target=start_ollama)\n",
    "ollama_thread.daemon = True\n",
    "ollama_thread.start()\n",
    "\n",
    "# Wait for Ollama HTTP API to be ready\n",
    "def wait_for_ollama(timeout=60):\n",
    "    for i in range(timeout):\n",
    "        try:\n",
    "            r = requests.get(\"http://localhost:11434\")\n",
    "            if r.status_code in [200, 404]:\n",
    "                print(f\"‚úÖ Ollama is up (after {i+1}s).\")\n",
    "                return True\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "        print(f\"‚è≥ Waiting for Ollama to start... {i+1}s\")\n",
    "        time.sleep(1)\n",
    "    print(\"‚ùå Ollama did not start in time.\")\n",
    "    return False\n",
    "\n",
    "# Wait for Ollama to start\n",
    "if not wait_for_ollama():\n",
    "    raise RuntimeError(\"Failed to start Ollama server\")\n",
    "\n",
    "# Pull the model\n",
    "print(f\"üì• Downloading model {MODEL_NAME}...\")\n",
    "!ollama pull {MODEL_NAME}\n",
    "\n",
    "# Setup Cloudflare tunnel\n",
    "print(\"üåê Setting up Cloudflare tunnel...\")\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
    "!chmod +x cloudflared\n",
    "\n",
    "# Run cloudflared tunnel in background and get the public URL\n",
    "cloudflared_proc = subprocess.Popen(\n",
    "    ['./cloudflared', 'tunnel', '--url', 'http://localhost:11434', '--no-autoupdate'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Extract the public URL\n",
    "public_url = None\n",
    "for _ in range(30):  # Wait up to 30 seconds for the URL\n",
    "    line = cloudflared_proc.stdout.readline().strip()\n",
    "    print(line)\n",
    "    match = re.search(r'(https://.*\\.trycloudflare\\.com)', line)\n",
    "    if match:\n",
    "        public_url = match.group(1)\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "if public_url:\n",
    "    print(f\"\\n‚úÖ Public URL for Ollama API:\\n{public_url}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå Could not find public Cloudflare URL.\")\n",
    "\n",
    "# Test the connection with a simple query\n",
    "print(\"üß™ Testing connection with a quick query...\")\n",
    "data = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"prompt\": \"Say hello in one sentence:\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{public_url}/api/generate\", json=data, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"‚úÖ Connection test successful! Response: {response.json()['response']}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è API returned status code {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Gradio Chat Interface\n",
    "\n",
    "Now we'll create an enhanced chat interface with additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import markdown\n",
    "from datetime import datetime\n",
    "\n",
    "# Keep track of conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Cache for system stats\n",
    "system_stats = {\"last_updated\": None, \"data\": {}}\n",
    "\n",
    "# Function to format messages for the API\n",
    "def format_prompt(message: str, history: List[List[str]], system_prompt: str = None) -> str:\n",
    "    \"\"\"Format the message and history into a prompt for the API.\"\"\"\n",
    "    formatted_prompt = \"\"\n",
    "    \n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        formatted_prompt = f\"{system_prompt}\\n\\n\"\n",
    "    \n",
    "    # Add conversation history\n",
    "    for user_msg, assistant_msg in history:\n",
    "        formatted_prompt += f\"User: {user_msg}\\n\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "    \n",
    "    # Add the current message\n",
    "    formatted_prompt += f\"User: {message}\\n\\nAssistant: \"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "# Function to chat with the LLM\n",
    "def chat_with_settings(message: str, \n",
    "                      history: List[List[str]], \n",
    "                      system_prompt: str,\n",
    "                      temp: float, \n",
    "                      top_p_val: float, \n",
    "                      max_tok: int,\n",
    "                      context_window: int):\n",
    "    \"\"\"Chat with the LLM using custom settings.\"\"\"\n",
    "    \n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    try:\n",
    "        # Format the prompt with conversation history\n",
    "        formatted_prompt = format_prompt(message, history, system_prompt)\n",
    "        \n",
    "        # Prepare data for API request\n",
    "        data = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": formatted_prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": temp,\n",
    "                \"top_p\": top_p_val,\n",
    "                \"num_predict\": max_tok,\n",
    "                \"num_ctx\": context_window\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Check if cloudflared process is still running\n",
    "        if cloudflared_proc.poll() is None:\n",
    "            # Start timer to measure response time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Send request to Ollama API\n",
    "            response = requests.post(f\"{public_url}/api/generate\", json=data, timeout=120)\n",
    "            \n",
    "            # Calculate response time\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                bot_response = result.get('response', 'No response generated.')\n",
    "                \n",
    "                # Get token metrics if available\n",
    "                eval_count = result.get('eval_count', 0)\n",
    "                prompt_eval_count = result.get('prompt_eval_count', 0)\n",
    "                \n",
    "                # Format metrics for display\n",
    "                metrics = f\"\\n\\n---\\n*Generated {eval_count} tokens in {response_time:.2f}s ({eval_count/response_time:.1f} tokens/s)*\"\n",
    "                \n",
    "                # Update history with the exchange\n",
    "                history.append([message, bot_response + metrics])\n",
    "                conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "                conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "                \n",
    "                return \"\", history\n",
    "            else:\n",
    "                error_msg = f\"‚ùå API Error: {response.status_code} - {response.text}\"\n",
    "                history.append([message, error_msg])\n",
    "                return \"\", history\n",
    "        else:\n",
    "            error_msg = \"‚ùå Cloudflared tunnel is not running. Please restart the tunnel.\"\n",
    "            history.append([message, error_msg])\n",
    "            return \"\", history\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        error_msg = \"‚è±Ô∏è Request timed out. The model might be taking too long to respond.\"\n",
    "        history.append([message, error_msg])\n",
    "        return \"\", history\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        error_msg = \"üîå Connection error. Please check if the tunnel is still active.\"\n",
    "        history.append([message, error_msg])\n",
    "        return \"\", history\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\"\n",
    "        history.append([message, error_msg])\n",
    "        return \"\", history\n",
    "\n",
    "# Function to clear chat history\n",
    "def clear_chat():\n",
    "    \"\"\"Clear the chat history.\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    return [], []\n",
    "\n",
    "# Function to retry the last message\n",
    "def retry_last_message(history, system_prompt, temp, top_p_val, max_tok, context_window):\n",
    "    \"\"\"Retry the last message with potentially new settings.\"\"\"\n",
    "    if not history:\n",
    "        return history\n",
    "    \n",
    "    last_user_msg = history[-1][0]\n",
    "    # Remove the last exchange\n",
    "    history = history[:-1]\n",
    "    # Resend the last user message\n",
    "    _, updated_history = chat_with_settings(last_user_msg, history, system_prompt, temp, top_p_val, max_tok, context_window)\n",
    "    return updated_history\n",
    "\n",
    "# Function to get model info\n",
    "def get_model_info():\n",
    "    \"\"\"Get information about the current model.\"\"\"\n",
    "    try:\n",
    "        if cloudflared_proc.poll() is None:\n",
    "            response = requests.get(f\"{public_url}/api/tags\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json().get('models', [])\n",
    "                current_model = next((m for m in models if m['name'] == MODEL_NAME), None)\n",
    "                if current_model:\n",
    "                    return f\"üìã **Current Model:** {MODEL_NAME}\\nüìè **Size:** {current_model.get('size', 'Unknown')} bytes\\nüè∑Ô∏è **Modified:** {current_model.get('modified_at', 'Unknown')}\"\n",
    "        return f\"üìã **Current Model:** {MODEL_NAME}\\n‚ö†Ô∏è **Status:** Model info unavailable\"\n",
    "    except Exception as e:\n",
    "        return f\"üìã **Current Model:** {MODEL_NAME}\\n‚ùå **Status:** Cannot retrieve model info ({str(e)})\"\n",
    "\n",
    "# Function to save conversation\n",
    "def save_conversation(history):\n",
    "    \"\"\"Save the current conversation to a JSON file.\"\"\"\n",
    "    if not history:\n",
    "        return \"‚ùå No conversation to save.\"\n",
    "    \n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"conversation_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(conversation_history, f, indent=2)\n",
    "        \n",
    "        return f\"‚úÖ Conversation saved to {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error saving conversation: {str(e)}\"\n",
    "\n",
    "# Custom CSS for better styling\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 1200px !important;\n",
    "    margin: auto;\n",
    "}\n",
    "\n",
    ".chat-message {\n",
    "    padding: 10px;\n",
    "    margin: 5px 0;\n",
    "    border-radius: 10px;\n",
    "}\n",
    "\n",
    ".user-message {\n",
    "    background-color: #e3f2fd;\n",
    "    margin-left: 20%;\n",
    "}\n",
    "\n",
    ".bot-message {\n",
    "    background-color: #f5f5f5;\n",
    "    margin-right: 20%;\n",
    "}\n",
    "\n",
    ".message-header {\n",
    "    font-weight: bold;\n",
    "    margin-bottom: 5px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üé® Setting up Gradio interface...\")\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=custom_css, title=\"ü§ñ Enhanced LLM Chat\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ü§ñ Enhanced LLM Chat Interface\n",
    "        \n",
    "        Chat with your locally running LLM via Ollama. The model is accessible through a Cloudflare tunnel.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            # Main chat interface\n",
    "            chatbot = gr.Chatbot(\n",
    "                [],\n",
    "                elem_id=\"chatbot\",\n",
    "                bubble_full_width=False,\n",
    "                height=500,\n",
    "                show_label=False\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"Type your message here...\",\n",
    "                    container=False,\n",
    "                    scale=4,\n",
    "                    show_label=False\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send üì§\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"Clear Chat üóëÔ∏è\", scale=1)\n",
    "                retry_btn = gr.Button(\"Retry Last üîÑ\", scale=1)\n",
    "                save_btn = gr.Button(\"Save Chat üíæ\", scale=1)\n",
    "            \n",
    "            # Status message area\n",
    "            status_msg = gr.Markdown(\"\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            # Sidebar with model info and controls\n",
    "            gr.Markdown(\"### üîß Model Information\")\n",
    "            model_info = gr.Markdown(get_model_info())\n",
    "            refresh_info_btn = gr.Button(\"Refresh Model Info üîÑ\", size=\"sm\")\n",
    "            \n",
    "            gr.Markdown(\"### üß† System Prompt\")\n",
    "            system_prompt = gr.Textbox(\n",
    "                placeholder=\"Optional: Set a system prompt to guide the model's behavior\",\n",
    "                label=\"System Prompt\",\n",
    "                value=\"You are a helpful AI assistant.\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
    "            with gr.Accordion(\"Generation Parameters\", open=True):\n",
    "                temperature = gr.Slider(\n",
    "                    minimum=0.1,\n",
    "                    maximum=2.0,\n",
    "                    value=0.7,\n",
    "                    step=0.1,\n",
    "                    label=\"Temperature\",\n",
    "                    info=\"Controls randomness (lower = more focused)\"\n",
    "                )\n",
    "                \n",
    "                top_p = gr.Slider(\n",
    "                    minimum=0.1,\n",
    "                    maximum=1.0,\n",
    "                    value=0.9,\n",
    "                    step=0.05,\n",
    "                    label=\"Top P\",\n",
    "                    info=\"Controls diversity (lower = more focused)\"\n",
    "                )\n",
    "                \n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=100,\n",
    "                    maximum=4096,\n",
    "                    value=2048,\n",
    "                    step=100,\n",
    "                    label=\"Max Tokens\",\n",
    "                    info=\"Maximum response length\"\n",
    "                )\n",
    "                \n",
    "                context_window = gr.Slider(\n",
    "                    minimum=1024,\n",
    "                    maximum=16384,\n",
    "                    value=8192,\n",
    "                    step=1024,\n",
    "                    label=\"Context Window\",\n",
    "                    info=\"Maximum context length\"\n",
    "                )\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(\n",
    "        chat_with_settings,\n",
    "        inputs=[msg, chatbot, system_prompt, temperature, top_p, max_tokens, context_window],\n",
    "        outputs=[msg, chatbot]\n",
    "    )\n",
    "    \n",
    "    send_btn.click(\n",
    "        chat_with_settings,\n",
    "        inputs=[msg, chatbot, system_prompt, temperature, top_p, max_tokens, context_window],\n",
    "        outputs=[msg, chatbot]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        clear_chat,\n",
    "        outputs=[chatbot, msg]\n",
    "    )\n",
    "    \n",
    "    retry_btn.click(\n",
    "        retry_last_message,\n",
    "        inputs=[chatbot, system_prompt, temperature, top_p, max_tokens, context_window],\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    refresh_info_btn.click(\n",
    "        get_model_info,\n",
    "        outputs=[model_info]\n",
    "    )\n",
    "    \n",
    "    save_btn.click(\n",
    "        save_conversation,\n",
    "        inputs=[chatbot],\n",
    "        outputs=[status_msg]\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Gradio interface created successfully!\")\n",
    "print(\"üöÄ Launching the web interface...\")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(\n",
    "    share=True,  # Create a public link\n",
    "    server_name=\"0.0.0.0\",  # Allow external connections\n",
    "    server_port=7860,  # Use a specific port\n",
    "    show_error=True,\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use:\n",
    "\n",
    "1. **Click the public Gradio link** that appears above\n",
    "2. **Type your questions** in the chat input field\n",
    "3. **Adjust settings** in the right sidebar:\n",
    "   - **System Prompt**: Sets the personality/behavior of the AI\n",
    "   - **Temperature**: Controls creativity (0.1 = focused, 2.0 = creative)\n",
    "   - **Top P**: Controls diversity (0.1 = narrow, 1.0 = diverse)\n",
    "   - **Max Tokens**: Controls maximum response length\n",
    "   - **Context Window**: Sets how much conversation history to include\n",
    "4. **Use the buttons**:\n",
    "   - **Send**: Submit your message\n",
    "   - **Clear Chat**: Reset conversation\n",
    "   - **Retry Last**: Regenerate the last response with current settings\n",
    "   - **Save Chat**: Export conversation as JSON\n",
    "   - **Refresh Model Info**: Update model status"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "colab_ui_enhanced.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
