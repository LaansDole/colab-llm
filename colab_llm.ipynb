{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG0UDdk86dxb"
      },
      "outputs": [],
      "source": [
        "# Model selection\n",
        "MODEL_NAME = \"maryasov/qwen2.5-coder-cline:7b-instruct-q8_0\"\n",
        "%env OLLAMA_CONTEXT_LENGTH=16384\n",
        "%env OLLAMA_HOST=0.0.0.0\n",
        "%env OLLAMA_KEEP_ALIVE=-1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VskPhA1M6h8j"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y lshw pciutils\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(f\"\\nüß† Available RAM: {ram_gb:.1f} GB\")\n",
        "print(\"‚úÖ High-RAM runtime!\" if ram_gb >= 20 else \"‚ùå Not a high-RAM runtime.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J83WxdLL6k75"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uR5FDlu6nav"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "# Start ollama serve in a background thread\n",
        "def start_ollama():\n",
        "    subprocess.call(['ollama', 'serve'])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Pull model (this also verifies Ollama CLI is ready)\n",
        "!ollama pull {MODEL_NAME}\n",
        "\n",
        "# Wait for Ollama HTTP API to be ready\n",
        "def wait_for_ollama(timeout=60):\n",
        "    for i in range(timeout):\n",
        "        try:\n",
        "            r = requests.get(\"http://localhost:11434\")\n",
        "            if r.status_code in [200, 404]:\n",
        "                print(f\"‚úÖ Ollama is up (after {i+1}s).\")\n",
        "                return\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            pass\n",
        "        print(f\"‚è≥ Waiting for Ollama to start... {i+1}s\")\n",
        "        time.sleep(1)\n",
        "    raise RuntimeError(\"‚ùå Ollama did not start in time.\")\n",
        "\n",
        "wait_for_ollama()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XUWhodx6pTh"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVmbIF6b6qsk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Run cloudflared tunnel in background and get the public URL\n",
        "cloudflared_proc = subprocess.Popen(\n",
        "    ['./cloudflared', 'tunnel', '--url', 'http://localhost:11434', '--no-autoupdate'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "public_url = None\n",
        "for line in cloudflared_proc.stdout:\n",
        "    print(line.strip())\n",
        "    match = re.search(r'(https://.*\\.trycloudflare\\.com)', line)\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        break\n",
        "\n",
        "if public_url:\n",
        "    print(f\"\\n‚úÖ Public URL for Ollama:\\n{public_url}\")\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå Could not find public Cloudflare URL.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4oAfEAG6sfb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess # Import subprocess if not already imported in this cell\n",
        "\n",
        "data = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"prompt\": \"Question: What is the capital of Japan?\\nAnswer:\",\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "# Check if the cloudflared process is still running\n",
        "if cloudflared_proc.poll() is None:\n",
        "    try:\n",
        "        response = requests.post(f\"{public_url}/api/generate\", json=data)\n",
        "        # Prettify the JSON output before printing\n",
        "        print(json.dumps(response.json()['response'], indent=4))\n",
        "    except requests.exceptions.ConnectionError as e:\n",
        "        print(f\"‚ùå Connection Error: Could not connect to {public_url}. The Cloudflare tunnel might be unstable or the Ollama service is not reachable via the tunnel.\")\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"‚ùå Cloudflared tunnel process is not running. The public URL is likely invalid.\")\n",
        "    # You might want to add logic here to attempt restarting the tunnel or raise an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Gradio for creating the web UI\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Gradio Web UI for Chat Interface\n",
        "\n",
        "This section implements a **Gradio-based chat interface** that provides:\n",
        "\n",
        "### ‚ú® Features:\n",
        "- üí¨ **Interactive Chat**: Real-time conversation with your local LLM\n",
        "- üéõÔ∏è **Adjustable Parameters**: Control temperature, top-p, and max tokens\n",
        "- üîÑ **Retry Functionality**: Retry the last message with different settings\n",
        "- üßπ **Clear Chat**: Start fresh conversations anytime\n",
        "- üìä **Model Information**: View current model details and status\n",
        "- üåê **Public Access**: Shareable link for external access\n",
        "- üé® **Modern UI**: Clean, responsive design with proper styling\n",
        "\n",
        "### üöÄ Usage:\n",
        "1. Run the cells below to install Gradio and set up the interface\n",
        "2. The interface will be accessible via a public Gradio link\n",
        "3. Use the chat interface to interact with your LLM\n",
        "4. Adjust settings in the sidebar as needed\n",
        "\n",
        "### üîó Integration:\n",
        "The interface connects to your existing **Ollama server** through the **Cloudflare tunnel**, providing seamless access to your local LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Chat history storage\n",
        "chat_history = []\n",
        "\n",
        "def chat_with_llm(message: str, history: List[List[str]]) -> Tuple[str, List[List[str]]]:\n",
        "    \"\"\"\n",
        "    Send a message to the LLM and return the response along with updated history.\n",
        "    \"\"\"\n",
        "    if not message.strip():\n",
        "        return \"\", history\n",
        "    \n",
        "    try:\n",
        "        # Prepare the data for Ollama API\n",
        "        data = {\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"prompt\": message,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.9,\n",
        "                \"max_tokens\": 2048\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Check if cloudflared process is still running\n",
        "        if cloudflared_proc.poll() is None:\n",
        "            # Send request to Ollama API\n",
        "            response = requests.post(f\"{public_url}/api/generate\", json=data, timeout=120)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                bot_response = result.get('response', 'No response generated.')\n",
        "                \n",
        "                # Update history\n",
        "                history.append([message, bot_response])\n",
        "                \n",
        "                return \"\", history\n",
        "            else:\n",
        "                error_msg = f\"‚ùå API Error: {response.status_code} - {response.text}\"\n",
        "                history.append([message, error_msg])\n",
        "                return \"\", history\n",
        "        else:\n",
        "            error_msg = \"‚ùå Cloudflared tunnel is not running. Please restart the tunnel.\"\n",
        "            history.append([message, error_msg])\n",
        "            return \"\", history\n",
        "            \n",
        "    except requests.exceptions.Timeout:\n",
        "        error_msg = \"‚è±Ô∏è Request timed out. The model might be taking too long to respond.\"\n",
        "        history.append([message, error_msg])\n",
        "        return \"\", history\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        error_msg = \"üîå Connection error. Please check if the tunnel is still active.\"\n",
        "        history.append([message, error_msg])\n",
        "        return \"\", history\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Unexpected error: {str(e)}\"\n",
        "        history.append([message, error_msg])\n",
        "        return \"\", history\n",
        "\n",
        "def clear_chat():\n",
        "    \"\"\"\n",
        "    Clear the chat history.\n",
        "    \"\"\"\n",
        "    return [], []\n",
        "\n",
        "def get_model_info():\n",
        "    \"\"\"\n",
        "    Get information about the current model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if cloudflared_proc.poll() is None:\n",
        "            response = requests.get(f\"{public_url}/api/tags\", timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                models = response.json().get('models', [])\n",
        "                current_model = next((m for m in models if m['name'] == MODEL_NAME), None)\n",
        "                if current_model:\n",
        "                    return f\"üìã **Current Model:** {MODEL_NAME}\\nüìè **Size:** {current_model.get('size', 'Unknown')} bytes\\nüè∑Ô∏è **Modified:** {current_model.get('modified_at', 'Unknown')}\"\n",
        "        return f\"üìã **Current Model:** {MODEL_NAME}\\n‚ö†Ô∏è **Status:** Model info unavailable\"\n",
        "    except:\n",
        "        return f\"üìã **Current Model:** {MODEL_NAME}\\n‚ùå **Status:** Cannot retrieve model info\"\n",
        "\n",
        "# Custom CSS for better styling\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    max-width: 1200px !important;\n",
        "    margin: auto;\n",
        "}\n",
        "\n",
        ".chat-message {\n",
        "    padding: 10px;\n",
        "    margin: 5px 0;\n",
        "    border-radius: 10px;\n",
        "}\n",
        "\n",
        ".user-message {\n",
        "    background-color: #e3f2fd;\n",
        "    margin-left: 20%;\n",
        "}\n",
        "\n",
        ".bot-message {\n",
        "    background-color: #f5f5f5;\n",
        "    margin-right: 20%;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(\"üé® Setting up Gradio interface...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the Gradio interface\n",
        "with gr.Blocks(css=custom_css, title=\"ü§ñ Local LLM Chat\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # ü§ñ Local LLM Chat Interface\n",
        "        \n",
        "        Chat with your locally running LLM via Ollama. The model is accessible through a Cloudflare tunnel.\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            # Main chat interface\n",
        "            chatbot = gr.Chatbot(\n",
        "                [],\n",
        "                elem_id=\"chatbot\",\n",
        "                bubble_full_width=False,\n",
        "                height=500,\n",
        "                show_label=False\n",
        "            )\n",
        "            \n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"Type your message here...\",\n",
        "                    container=False,\n",
        "                    scale=4,\n",
        "                    show_label=False\n",
        "                )\n",
        "                send_btn = gr.Button(\"Send üì§\", scale=1, variant=\"primary\")\n",
        "            \n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"Clear Chat üóëÔ∏è\", scale=1)\n",
        "                retry_btn = gr.Button(\"Retry Last üîÑ\", scale=1)\n",
        "        \n",
        "        with gr.Column(scale=1):\n",
        "            # Sidebar with model info and controls\n",
        "            gr.Markdown(\"### üîß Model Information\")\n",
        "            model_info = gr.Markdown(get_model_info())\n",
        "            \n",
        "            gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
        "            \n",
        "            with gr.Accordion(\"Advanced Options\", open=False):\n",
        "                temperature = gr.Slider(\n",
        "                    minimum=0.1,\n",
        "                    maximum=2.0,\n",
        "                    value=0.7,\n",
        "                    step=0.1,\n",
        "                    label=\"Temperature\",\n",
        "                    info=\"Controls randomness (lower = more focused)\"\n",
        "                )\n",
        "                \n",
        "                top_p = gr.Slider(\n",
        "                    minimum=0.1,\n",
        "                    maximum=1.0,\n",
        "                    value=0.9,\n",
        "                    step=0.05,\n",
        "                    label=\"Top P\",\n",
        "                    info=\"Controls diversity (lower = more focused)\"\n",
        "                )\n",
        "                \n",
        "                max_tokens = gr.Slider(\n",
        "                    minimum=100,\n",
        "                    maximum=4096,\n",
        "                    value=2048,\n",
        "                    step=100,\n",
        "                    label=\"Max Tokens\",\n",
        "                    info=\"Maximum response length\"\n",
        "                )\n",
        "            \n",
        "            refresh_info_btn = gr.Button(\"Refresh Model Info üîÑ\", size=\"sm\")\n",
        "    \n",
        "    # Define the enhanced chat function with settings\n",
        "    def chat_with_settings(message: str, history: List[List[str]], temp: float, top_p_val: float, max_tok: int):\n",
        "        if not message.strip():\n",
        "            return \"\", history\n",
        "        \n",
        "        try:\n",
        "            data = {\n",
        "                \"model\": MODEL_NAME,\n",
        "                \"prompt\": message,\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"temperature\": temp,\n",
        "                    \"top_p\": top_p_val,\n",
        "                    \"num_predict\": max_tok\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            if cloudflared_proc.poll() is None:\n",
        "                response = requests.post(f\"{public_url}/api/generate\", json=data, timeout=120)\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    bot_response = result.get('response', 'No response generated.')\n",
        "                    history.append([message, bot_response])\n",
        "                    return \"\", history\n",
        "                else:\n",
        "                    error_msg = f\"‚ùå API Error: {response.status_code}\"\n",
        "                    history.append([message, error_msg])\n",
        "                    return \"\", history\n",
        "            else:\n",
        "                error_msg = \"‚ùå Cloudflared tunnel is not running.\"\n",
        "                history.append([message, error_msg])\n",
        "                return \"\", history\n",
        "                \n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå Error: {str(e)}\"\n",
        "            history.append([message, error_msg])\n",
        "            return \"\", history\n",
        "    \n",
        "    def retry_last_message(history: List[List[str]], temp: float, top_p_val: float, max_tok: int):\n",
        "        if not history:\n",
        "            return history\n",
        "        \n",
        "        last_user_msg = history[-1][0]\n",
        "        # Remove the last exchange\n",
        "        history = history[:-1]\n",
        "        # Resend the last user message\n",
        "        _, updated_history = chat_with_settings(last_user_msg, history, temp, top_p_val, max_tok)\n",
        "        return updated_history\n",
        "    \n",
        "    # Event handlers\n",
        "    msg.submit(\n",
        "        chat_with_settings,\n",
        "        inputs=[msg, chatbot, temperature, top_p, max_tokens],\n",
        "        outputs=[msg, chatbot]\n",
        "    )\n",
        "    \n",
        "    send_btn.click(\n",
        "        chat_with_settings,\n",
        "        inputs=[msg, chatbot, temperature, top_p, max_tokens],\n",
        "        outputs=[msg, chatbot]\n",
        "    )\n",
        "    \n",
        "    clear_btn.click(\n",
        "        clear_chat,\n",
        "        outputs=[chatbot, chatbot]\n",
        "    )\n",
        "    \n",
        "    retry_btn.click(\n",
        "        retry_last_message,\n",
        "        inputs=[chatbot, temperature, top_p, max_tokens],\n",
        "        outputs=[chatbot]\n",
        "    )\n",
        "    \n",
        "    refresh_info_btn.click(\n",
        "        get_model_info,\n",
        "        outputs=[model_info]\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Gradio interface created successfully!\")\n",
        "print(\"üöÄ Launching the web interface...\")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(\n",
        "    share=True,  # Create a public link\n",
        "    server_name=\"0.0.0.0\",  # Allow external connections\n",
        "    server_port=7860,  # Use a specific port\n",
        "    show_error=True,\n",
        "    debug=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Gradio Interface Successfully Launched!\n",
        "\n",
        "### üì± How to Use:\n",
        "1. **Click the public Gradio link** that appears above\n",
        "2. **Type your questions** in the chat input field\n",
        "3. **Adjust settings** in the right sidebar if needed:\n",
        "   - **Temperature**: Controls creativity (0.1 = focused, 2.0 = creative)\n",
        "   - **Top P**: Controls diversity (0.1 = narrow, 1.0 = diverse)\n",
        "   - **Max Tokens**: Controls response length (100-4096)\n",
        "4. **Use the buttons**:\n",
        "   - üöÄ **Send**: Submit your message\n",
        "   - üóëÔ∏è **Clear Chat**: Reset conversation\n",
        "   - üîÑ **Retry Last**: Regenerate the last response\n",
        "   - üîÑ **Refresh Model Info**: Update model status\n",
        "\n",
        "### üí° Tips:\n",
        "- The interface maintains **chat history** during your session\n",
        "- **Lower temperature** = more consistent, focused responses\n",
        "- **Higher temperature** = more creative, varied responses\n",
        "- If you get errors, check that both **Ollama** and **Cloudflare tunnel** are still running\n",
        "\n",
        "### üîß Troubleshooting:\n",
        "- If the interface doesn't respond, scroll up to verify the **Cloudflare tunnel URL** is still active\n",
        "- Check the **Model Information** panel to ensure your model is loaded\n",
        "- Use **Retry Last** if a response seems incomplete or unsatisfactory"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
