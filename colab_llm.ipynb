{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG0UDdk86dxb"
      },
      "outputs": [],
      "source": [
        "# Model selection\n",
        "MODEL_NAME = \"maryasov/qwen2.5-coder-cline:7b-instruct-q8_0\"\n",
        "%env OLLAMA_CONTEXT_LENGTH=16384\n",
        "%env OLLAMA_HOST=0.0.0.0\n",
        "%env OLLAMA_KEEP_ALIVE=-1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VskPhA1M6h8j"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y lshw pciutils\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(f\"\\nüß† Available RAM: {ram_gb:.1f} GB\")\n",
        "print(\"‚úÖ High-RAM runtime!\" if ram_gb >= 20 else \"‚ùå Not a high-RAM runtime.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J83WxdLL6k75"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uR5FDlu6nav"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "# Start ollama serve in a background thread\n",
        "def start_ollama():\n",
        "    subprocess.call(['ollama', 'serve'])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Pull model (this also verifies Ollama CLI is ready)\n",
        "!ollama pull {MODEL_NAME}\n",
        "\n",
        "# Wait for Ollama HTTP API to be ready\n",
        "def wait_for_ollama(timeout=60):\n",
        "    for i in range(timeout):\n",
        "        try:\n",
        "            r = requests.get(\"http://localhost:11434\")\n",
        "            if r.status_code in [200, 404]:\n",
        "                print(f\"‚úÖ Ollama is up (after {i+1}s).\")\n",
        "                return\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            pass\n",
        "        print(f\"‚è≥ Waiting for Ollama to start... {i+1}s\")\n",
        "        time.sleep(1)\n",
        "    raise RuntimeError(\"‚ùå Ollama did not start in time.\")\n",
        "\n",
        "wait_for_ollama()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XUWhodx6pTh"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVmbIF6b6qsk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Run cloudflared tunnel in background and get the public URL\n",
        "cloudflared_proc = subprocess.Popen(\n",
        "    ['./cloudflared', 'tunnel', '--url', 'http://localhost:11434', '--no-autoupdate'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "public_url = None\n",
        "for line in cloudflared_proc.stdout:\n",
        "    print(line.strip())\n",
        "    match = re.search(r'(https://.*\\.trycloudflare\\.com)', line)\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        break\n",
        "\n",
        "if public_url:\n",
        "    print(f\"\\n‚úÖ Public URL for Ollama:\\n{public_url}\")\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå Could not find public Cloudflare URL.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4oAfEAG6sfb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess # Import subprocess if not already imported in this cell\n",
        "\n",
        "data = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"prompt\": \"Question: What is the capital of Japan?\\nAnswer:\",\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "# Check if the cloudflared process is still running\n",
        "if cloudflared_proc.poll() is None:\n",
        "    try:\n",
        "        response = requests.post(f\"{public_url}/api/generate\", json=data)\n",
        "        # Prettify the JSON output before printing\n",
        "        print(json.dumps(response.json()['response'], indent=4))\n",
        "    except requests.exceptions.ConnectionError as e:\n",
        "        print(f\"‚ùå Connection Error: Could not connect to {public_url}. The Cloudflare tunnel might be unstable or the Ollama service is not reachable via the tunnel.\")\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"‚ùå Cloudflared tunnel process is not running. The public URL is likely invalid.\")\n",
        "    # You might want to add logic here to attempt restarting the tunnel or raise an error."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
